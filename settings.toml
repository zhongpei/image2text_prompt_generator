[server]
port = 7869
host = '127.0.0.1'
enable_queue = true
queue_size = 10
show_api = true
debug = true

[chatglm]
model = "THUDM/chatglm-6b-int8" # THUDM/chatglm-6b-int4 THUDM/chatglm-6b-int8 THUDM/chatglm-6b
#model = "./models/chatglm-6b-int4" # THUDM/chatglm-6b-int4 THUDM/chatglm-6b-int8 THUDM/chatglm-6b
device = "cuda" # cpu mps cuda
enable_chat = false
local_files_only = false
