[server]
port = 7868 # 端口
host = '127.0.0.1' # 局域网访问需要改成 "0.0.0.0"
enable_queue = false # chat功能需要开启，如错误，需要关闭代理
queue_size = 10
show_api = false
debug = true

[api]
generate_num_return_sequences = 2 # 生成返回数量
generate_translate = true # 是否启用生成翻译功能

[image_tools]
enable = true # 是否启用图片处理功能
transparent_background_path = ".\\system\\python\\Scripts\\transparent-background.exe" # transparent-background.exe路径

[generator]
enable = true # 是否启用generator功能
device = "cuda" # cpu mps cuda

# models
microsoft_model = "microsoft/Promptist"
gpt2_650k_model = "Ar4ikov/gpt2-650k-stable-diffusion-prompt-generator"
gpt_neo_125m_model = "DrishtiSharma/StableDiffusion-Prompt-Generator-GPT-Neo-125M"
mj_model = "succinctly/text2image-prompt-generator"
local_files_only = false # 是否只使用本地模型

[translate]
enable = true # 是否启用翻译功能
device = "cuda" # cpu mps cuda
local_files_only = false # 是否只使用本地模型

zh2en_model = "Helsinki-NLP/opus-mt-zh-en"
en2zh_model = "Helsinki-NLP/opus-mt-en-zh"

# zh2en_model = "./models/opus-mt-zh-en"
# en2zh_model = "./models/opus-mt-en-zh"

[chatglm]
model = "THUDM/chatglm-6b-int4" # THUDM/chatglm-6b-int4 THUDM/chatglm-6b-int8 THUDM/chatglm-6b

# 本地模型 https://github.com/THUDM/ChatGLM-6B#从本地加载模型
# model = "./models/chatglm-6b-int4"

device = "cuda" # cpu mps cuda
enable_chat = false # 是否启用聊天功能
local_files_only = false # 是否只使用本地模型