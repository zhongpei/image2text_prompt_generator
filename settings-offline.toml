[server]
port = 7868 # 端口
host = '127.0.0.1' # 局域网访问需要改成 "0.0.0.0"
enable_queue = true # chat功能需要开启，如错误，需要关闭代理
queue_size = 10
show_api = false
debug = true


[api]
generate_num_return_sequences = 2 # 生成返回数量
generate_translate = true # 是否启用生成翻译功能

[image_tools]
enable = true # 是否启用图片处理功能
device = "cuda" # cpu mps cuda
transparent_background_model_path = ".\\models\\transparent-background\\ckpt_base.pth" # transparent-background模型路径


[generator]
enable = true # 是否启用generator功能
device = "cuda" # cpu mps cuda
fix_sd_prompt = true # 是否修复sd prompt
# models
microsoft_model = ".\\models\\Promptist"
gpt2_650k_model = ".\\models\\gpt2-650k-stable-diffusion-prompt-generator"
gpt_neo_125m_model = ".\\models\\StableDiffusion-Prompt-Generator-GPT-Neo-125M"
mj_model = ".\\models\\text2image-prompt-generator"
local_files_only = true # 是否只使用本地模型

default_model_name = "microsoft" # microsoft gpt2_650k gpt_neo_125m mj
default_num_return_sequences = 4 # 生成返回数量

[clip]
default_model_name = "vit_l_14" # vit_h_14 vit_l_14
default_model_type = "fast" # fast best classic negative

[git]
model = "microsoft/git-large-coco" # "microsoft/git-large-coco"

[wd14]
default_model_name="ConvNextV2"
default_general_threshold=0.35
default_character_threshold=0.85
local_files_only = true # 是否只使用本地模型
model_dir = ".\\models"

[translate]
enable = true # 是否启用翻译功能
device = "cuda" # cpu mps cuda
local_files_only = true # 是否只使用本地模型
zh2en_model = ".\\models\\opus-mt-zh-en"
en2zh_model = ".\\models\\opus-mt-en-zh"

cache_dir = "./data/translate_cache" # 翻译缓存目录


[chatglm]
# 本地模型 https://github.com/THUDM/ChatGLM-6B#从本地加载模型
model = ".\\models\\chatglm-6b-int4" # ./chatglm-6b-int4 ./chatglm-6b-int8 ./chatglm-6b
device = "cuda" # cpu mps cuda
enable_chat = true # 是否启用聊天功能
local_files_only = true # 是否只使用本地模型
